# -*- coding: utf-8 -*-
"""NLP Project (Team 5)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WlAvaWAndFU6CH6Gn0HH5PjEtfHeJ9NB
"""

from google.colab import files

uploaded = files.upload()

import pandas as pd

# Load the uploaded CSV file
df = pd.read_csv("Suicide_Detection_Sample.csv")

# Preview the data
df.head()

# ðŸ§  15 Mental Health Questions
questions = [
    "What are the early warning signs of suicidal thoughts in teenagers?",
    "How can I help a friend who talks about self-harm?",
    "What are the common causes of suicidal ideation in young adults?",
    "How do mental health professionals assess suicide risk?",
    "What are the immediate steps to take if someone is suicidal?",
    "Can social media use increase the risk of depression or suicide?",
    "How can schools and universities prevent student suicides?",
    "Is it true that talking about suicide can make someone more likely to act on it?",
    "What mental health resources are available for someone in crisis?",
    "How can family members support someone recovering from a suicide attempt?",
    "What is the role of therapy in suicide prevention?",
    "How does bullying impact mental health and suicidal behavior in teens?",
    "Are there specific signs in social media posts that indicate suicide risk?",
    "What medications are commonly used to treat suicidal depression?",
    "How do cultural factors influence attitudes toward suicide and mental health?"
]

!pip install -U langchain langchain-community

!pip install chromadb sentence-transformers transformers

# ðŸ”Œ Load embedding model & vectorstore
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="mental_health_db", embedding_function=embedding_function)

# Load instruction-tuned model (LaMini-Flan)
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline
from langchain.llms import HuggingFacePipeline

model_name = "MBZUAI/LaMini-Flan-T5-783M"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

text_gen_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.95,
    do_sample=True  # <- enables temperature & top_p to take effect
)

llm = HuggingFacePipeline(pipeline=text_gen_pipeline)

# ðŸ”— Build RAG Chain (limit to 2 retrieved docs)
from langchain.chains import RetrievalQA
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=False
)

# Run questions through RAG

results = []
for i, question in enumerate(questions, 1):
    try:
        response = rag_chain({"query": question})
        print(f"Q{i}: {question}")
        print(f"A{i}: {response['result']}\n")
        results.append({"Question": question, "Answer": response["result"]})
    except Exception as e:
        print(f"Error on Q{i}: {question}\n{str(e)}\n")
        results.append({"Question": question, "Answer": f"Error: {str(e)}"})

lamini_scoring = [
    {"Question": "Q1", "Answer": "Context not available", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Acknowledged lack of info, no hallucination"},
    {"Question": "Q2", "Answer": "Vague but acknowledges help", "Factual": 1, "Relevance": 2, "Empathy": 2, "Justification": "Some effort to help, not detailed"},
    {"Question": "Q3", "Answer": "No info in context", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Safe fallback"},
    {"Question": "Q4", "Answer": "Doesn't answer", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Unclear, lacks context relevance"},
    {"Question": "Q5", "Answer": "No steps provided", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Too vague"},
    {"Question": "Q6", "Answer": "Says not enough info", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Safe fallback"},
    {"Question": "Q7", "Answer": "Vague", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Minimal info"},
    {"Question": "Q8", "Answer": "Refutes myth", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Correct stance"},
    {"Question": "Q9", "Answer": "Therapy + vague", "Factual": 1, "Relevance": 1, "Empathy": 2, "Justification": "Needs more detail"},
    {"Question": "Q10", "Answer": "Random terms", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Lacks clarity"},
    {"Question": "Q11", "Answer": "Correct role of therapy", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Concise and informative"},
    {"Question": "Q12", "Answer": "Says yes", "Factual": 0, "Relevance": 1, "Empathy": 1, "Justification": "Oversimplified"},
    {"Question": "Q13", "Answer": "No specific signs", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Appropriate fallback"},
    {"Question": "Q14", "Answer": "No info", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Appropriate fallback again"},
    {"Question": "Q15", "Answer": "Cultural influence mentioned", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Appropriate and safe"},
]

df_lamini_scored = pd.DataFrame(lamini_scoring)
df_lamini_scored

# LaMini scores
lamini_eval = {
    "Question": [f"Q{i}" for i in range(1, 16)],
    "Factual Accuracy": [2,1,2,1,1,2,2,2,1,1,2,0,2,2,2],
    "Relevance":        [2,2,2,1,1,2,1,2,1,1,2,1,2,2,2],
    "Empathy":          [2,2,2,1,1,2,1,2,2,1,2,1,2,2,2]
}

# Create DataFrame and save to CSV
df_lamini = pd.DataFrame(lamini_eval)
df_lamini.to_csv("LaMini_Eval.csv", index=False)

# Download the file
from google.colab import files
files.download("LaMini_Eval.csv")



from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.llms import HuggingFacePipeline

# Load flan-t5-large model + tokenizer
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Create a pipeline for text2text generation
flan_large_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.95,
    do_sample=True  # allows sampling
)

# Wrap with LangChain pipeline
llm_flan_large = HuggingFacePipeline(pipeline=flan_large_pipeline)

from langchain.chains import RetrievalQA

rag_chain_flan_large = RetrievalQA.from_chain_type(
    llm=llm_flan_large,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=False
)

flan_large_results = []
for i, question in enumerate(questions, 1):
    try:
        response = rag_chain_flan_large({"query": question})
        print(f"FLAN Q{i}: {question}")
        print(f"A{i}: {response['result']}\n")
        flan_large_results.append({"Question": question, "Answer": response["result"]})
    except Exception as e:
        flan_large_results.append({"Question": question, "Answer": f"Error: {str(e)}"})

flan_scoring = [
    {"Question": "Q1", "Answer": "cries of terror", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Too vague and not supportive"},
    {"Question": "Q2", "Answer": "Tell them to never self-harm again", "Factual": 1, "Relevance": 1, "Empathy": 0, "Justification": "Commanding tone, lacks empathy"},
    {"Question": "Q3", "Answer": "suicidal ideation", "Factual": 0, "Relevance": 1, "Empathy": 0, "Justification": "Repeats input without insight"},
    {"Question": "Q4", "Answer": "not enough information", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Accurately acknowledges missing info"},
    {"Question": "Q5", "Answer": "Tell them to stop", "Factual": 1, "Relevance": 1, "Empathy": 0, "Justification": "Insensitive response"},
    {"Question": "Q6", "Answer": "yes", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Concise and accurate"},
    {"Question": "Q7", "Answer": "Provide counseling", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Relevant and practical"},
    {"Question": "Q8", "Answer": "Yes", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Valid myth-busting response"},
    {"Question": "Q9", "Answer": "a mental health professional", "Factual": 2, "Relevance": 2, "Empathy": 1, "Justification": "Accurate but too brief"},
    {"Question": "Q10", "Answer": "Helpful", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Generic and uninformative"},
    {"Question": "Q11", "Answer": "a form of help", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Too vague to be meaningful"},
    {"Question": "Q12", "Answer": "affects their ability to concentrate", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Correct and empathetic"},
    {"Question": "Q13", "Answer": "no", "Factual": 1, "Relevance": 2, "Empathy": 1, "Justification": "Too brief; could mislead"},
    {"Question": "Q14", "Answer": "None", "Factual": 1, "Relevance": 1, "Empathy": 1, "Justification": "Incorrect medically"},
    {"Question": "Q15", "Answer": "Same as question", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Repetition, but accurate"}
]

df_flan_scored = pd.DataFrame(flan_scoring)
df_flan_scored

# Scored responses from FLAN-T5-Large (based on what you shared)
flan_eval = {
    "Question": [f"Q{i}" for i in range(1, 16)],
    "Factual Accuracy": [1, 1, 0, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2],
    "Relevance":        [1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2],
    "Empathy":          [1, 0, 0, 2, 0, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2]
}

# Create and save DataFrame
df_flan = pd.DataFrame(flan_eval)
df_flan.to_csv("FlanT5_Eval.csv", index=False)

# Trigger download
from google.colab import files
files.download("FlanT5_Eval.csv")



from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.llms import HuggingFacePipeline

model_name = "declare-lab/flan-alpaca-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

alpaca_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.95,
    do_sample=True
)

llm_alpaca = HuggingFacePipeline(pipeline=alpaca_pipeline)

from langchain.chains import RetrievalQA

rag_chain_alpaca = RetrievalQA.from_chain_type(
    llm=llm_alpaca,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=False
)

alpaca_results = []

for i, question in enumerate(questions, 1):
    try:
        response = rag_chain_alpaca({"query": question})
        print(f"ALPACA Q{i}: {question}")
        print(f"A{i}: {response['result']}\n")
        alpaca_results.append({
            "Question": question,
            "Answer": response["result"]
        })
    except Exception as e:
        print(f"Error on Q{i}: {question} â€” {str(e)}\n")
        alpaca_results.append({
            "Question": question,
            "Answer": f"Error: {str(e)}"
        })

alpaca_scoring = [
    {"Question": "Q1", "Answer": "Calmness, anxiety, isolation", "Factual": 1, "Relevance": 1, "Empathy": 2, "Justification": "Some signs are inaccurate, but empathetic"},
    {"Question": "Q2", "Answer": "Stay positive and focused", "Factual": 1, "Relevance": 1, "Empathy": 2, "Justification": "General help, not specific to self-harm"},
    {"Question": "Q3", "Answer": "Lack of sleep, social issues", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Detailed and clinically sound"},
    {"Question": "Q4", "Answer": "Describes psych assessment methods", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Professional and complete"},
    {"Question": "Q5", "Answer": "Rest, talk to friend, support", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Good emergency advice"},
    {"Question": "Q6", "Answer": "Loneliness, dissatisfaction, etc.", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Balanced explanation"},
    {"Question": "Q7", "Answer": "Detailed support methods", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Covers multiple approaches"},
    {"Question": "Q8", "Answer": "Talking helps reduce risk", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Supports open conversation"},
    {"Question": "Q9", "Answer": "Counseling, support groups", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Well-suggested options"},
    {"Question": "Q10", "Answer": "Emotional support, positivity", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Comforting and practical"},
    {"Question": "Q11", "Answer": "Reduces stress, boosts self-esteem", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Accurate and sensitive"},
    {"Question": "Q12", "Answer": "Fear, isolation, shame", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Comprehensive and emotional"},
    {"Question": "Q13", "Answer": "Guilt, shame, lack of empathy", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Valid indicators listed"},
    {"Question": "Q14", "Answer": "SSRIs, antipsychotics", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Correct medication types"},
    {"Question": "Q15", "Answer": "Coping, beliefs, values", "Factual": 2, "Relevance": 2, "Empathy": 2, "Justification": "Nuanced and insightful"}
]

df_alpaca_scored = pd.DataFrame(alpaca_scoring)
df_alpaca_scored

# Flan-Alpaca evaluation scores based on your responses
alpaca_eval = {
    "Question": [f"Q{i}" for i in range(1, 16)],
    "Factual Accuracy": [1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
    "Relevance":        [1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
    "Empathy":          [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
}

# Create DataFrame and save
df_alpaca = pd.DataFrame(alpaca_eval)
df_alpaca.to_csv("Alpaca_Eval.csv", index=False)

# Optional: download it from Colab
from google.colab import files
files.download("Alpaca_Eval.csv")





from google.colab import files

uploaded = files.upload()

df_lamini = pd.read_csv("LaMini_Eval.csv")
df_lamini

from google.colab import files

uploaded = files.upload()

df_flan = pd.read_csv("FlanT5_Eval.csv")
df_flan

from google.colab import files

uploaded = files.upload()

df_alpaca = pd.read_csv("Alpaca_Eval.csv")
df_alpaca

# Add model name
df_lamini["Model"] = "LaMini"
df_flan["Model"]   = "Flan-T5"
df_alpaca["Model"] = "Flan-Alpaca"

# Combine all
df_all = pd.concat([df_lamini, df_flan, df_alpaca], ignore_index=True)

# Group by model and get average scores
df_avg = df_all.groupby("Model")[["Factual Accuracy", "Relevance", "Empathy"]].mean().round(2)

# Display the score table
print("Average Evaluation Scores by Model:")
display(df_avg)

import matplotlib.pyplot as plt

# Plot bar chart
df_avg.plot(kind="bar", figsize=(8, 5), title="Model Evaluation Comparison")
plt.ylabel("Average Score (0â€“2)")
plt.ylim(0, 2.1)
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

